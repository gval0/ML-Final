{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"12io49GWy-14yLtqBs2gimuIiUHH-DUrd","authorship_tag":"ABX9TyP18ql/fz9ReUVKBwjaPKQL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install mlflow"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"8F3PffIHIKYz","executionInfo":{"status":"ok","timestamp":1751560300905,"user_tz":-240,"elapsed":18108,"user":{"displayName":"Elene Gvalia","userId":"05599052994539163242"}},"outputId":"adc453c9-9ca7-4851-bc97-33a851e91512"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting mlflow\n","  Downloading mlflow-3.1.1-py3-none-any.whl.metadata (29 kB)\n","Collecting mlflow-skinny==3.1.1 (from mlflow)\n","  Downloading mlflow_skinny-3.1.1-py3-none-any.whl.metadata (30 kB)\n","Requirement already satisfied: Flask<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.1.1)\n","Collecting alembic!=1.10.0,<2 (from mlflow)\n","  Downloading alembic-1.16.2-py3-none-any.whl.metadata (7.3 kB)\n","Collecting docker<8,>=4.0.0 (from mlflow)\n","  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n","Collecting graphene<4 (from mlflow)\n","  Downloading graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\n","Collecting gunicorn<24 (from mlflow)\n","  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n","Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.10.0)\n","Requirement already satisfied: numpy<3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.0.2)\n","Requirement already satisfied: pandas<3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.2.2)\n","Requirement already satisfied: pyarrow<21,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (18.1.0)\n","Requirement already satisfied: scikit-learn<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.6.1)\n","Requirement already satisfied: scipy<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.15.3)\n","Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.0.41)\n","Requirement already satisfied: cachetools<7,>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (5.5.2)\n","Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (8.2.1)\n","Requirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (3.1.1)\n","Collecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny==3.1.1->mlflow)\n","  Downloading databricks_sdk-0.57.0-py3-none-any.whl.metadata (39 kB)\n","Requirement already satisfied: fastapi<1 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (0.115.14)\n","Requirement already satisfied: gitpython<4,>=3.1.9 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (3.1.44)\n","Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (8.7.0)\n","Collecting opentelemetry-api<3,>=1.9.0 (from mlflow-skinny==3.1.1->mlflow)\n","  Downloading opentelemetry_api-1.34.1-py3-none-any.whl.metadata (1.5 kB)\n","Collecting opentelemetry-sdk<3,>=1.9.0 (from mlflow-skinny==3.1.1->mlflow)\n","  Downloading opentelemetry_sdk-1.34.1-py3-none-any.whl.metadata (1.6 kB)\n","Requirement already satisfied: packaging<26 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (24.2)\n","Requirement already satisfied: protobuf<7,>=3.12.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (5.29.5)\n","Requirement already satisfied: pydantic<3,>=1.10.8 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (2.11.7)\n","Requirement already satisfied: pyyaml<7,>=5.1 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (6.0.2)\n","Requirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (2.32.3)\n","Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (0.5.3)\n","Requirement already satisfied: typing-extensions<5,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (4.14.0)\n","Requirement already satisfied: uvicorn<1 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (0.35.0)\n","Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic!=1.10.0,<2->mlflow) (1.1.3)\n","Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from docker<8,>=4.0.0->mlflow) (2.4.0)\n","Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (1.9.0)\n","Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (2.2.0)\n","Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (3.1.6)\n","Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (3.0.2)\n","Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (3.1.3)\n","Collecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow)\n","  Downloading graphql_core-3.2.6-py3-none-any.whl.metadata (11 kB)\n","Collecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow)\n","  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: python-dateutil<3,>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from graphene<4->mlflow) (2.9.0.post0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (1.3.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (4.58.4)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (1.4.8)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (11.2.1)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (3.2.3)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3->mlflow) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3->mlflow) (2025.2)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2->mlflow) (1.5.1)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2->mlflow) (3.6.0)\n","Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.2.3)\n","Requirement already satisfied: google-auth~=2.0 in /usr/local/lib/python3.11/dist-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.1->mlflow) (2.38.0)\n","Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi<1->mlflow-skinny==3.1.1->mlflow) (0.46.2)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython<4,>=3.1.9->mlflow-skinny==3.1.1->mlflow) (4.0.12)\n","Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==3.1.1->mlflow) (3.23.0)\n","Collecting opentelemetry-semantic-conventions==0.55b1 (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==3.1.1->mlflow)\n","  Downloading opentelemetry_semantic_conventions-0.55b1-py3-none-any.whl.metadata (2.5 kB)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.1.1->mlflow) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.1.1->mlflow) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.1.1->mlflow) (0.4.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow) (1.17.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.1.1->mlflow) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.1.1->mlflow) (3.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.1.1->mlflow) (2025.6.15)\n","Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn<1->mlflow-skinny==3.1.1->mlflow) (0.16.0)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==3.1.1->mlflow) (5.0.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.1->mlflow) (0.4.2)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.1->mlflow) (4.9.1)\n","Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi<1->mlflow-skinny==3.1.1->mlflow) (4.9.0)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi<1->mlflow-skinny==3.1.1->mlflow) (1.3.1)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.1->mlflow) (0.6.1)\n","Downloading mlflow-3.1.1-py3-none-any.whl (24.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.7/24.7 MB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading mlflow_skinny-3.1.1-py3-none-any.whl (1.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading alembic-1.16.2-py3-none-any.whl (242 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.7/242.7 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading docker-7.1.0-py3-none-any.whl (147 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading graphene-3.4.3-py2.py3-none-any.whl (114 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading databricks_sdk-0.57.0-py3-none-any.whl (733 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m733.8/733.8 kB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading graphql_core-3.2.6-py3-none-any.whl (203 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n","Downloading opentelemetry_api-1.34.1-py3-none-any.whl (65 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading opentelemetry_sdk-1.34.1-py3-none-any.whl (118 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.5/118.5 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading opentelemetry_semantic_conventions-0.55b1-py3-none-any.whl (196 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.2/196.2 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: gunicorn, graphql-core, opentelemetry-api, graphql-relay, docker, alembic, opentelemetry-semantic-conventions, graphene, databricks-sdk, opentelemetry-sdk, mlflow-skinny, mlflow\n","Successfully installed alembic-1.16.2 databricks-sdk-0.57.0 docker-7.1.0 graphene-3.4.3 graphql-core-3.2.6 graphql-relay-3.2.0 gunicorn-23.0.0 mlflow-3.1.1 mlflow-skinny-3.1.1 opentelemetry-api-1.34.1 opentelemetry-sdk-1.34.1 opentelemetry-semantic-conventions-0.55b1\n"]}]},{"cell_type":"code","execution_count":6,"metadata":{"id":"F60UyfWaH_Nv","executionInfo":{"status":"ok","timestamp":1751560756021,"user_tz":-240,"elapsed":2,"user":{"displayName":"Elene Gvalia","userId":"05599052994539163242"}}},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.preprocessing import StandardScaler, LabelEncoder\n","from sklearn.impute import SimpleImputer, KNNImputer\n","from sklearn.pipeline import Pipeline\n","from sklearn.compose import ColumnTransformer\n","from sklearn.base import BaseEstimator, TransformerMixin\n","import pickle\n","import joblib\n","from datetime import datetime, timedelta\n","import warnings\n","warnings.filterwarnings('ignore')\n","import mlflow\n","import mlflow.sklearn\n","from mlflow.models.signature import infer_signature\n","import os"]},{"cell_type":"code","source":["os.chdir('/content/drive/MyDrive/ml-final/ML-Final')"],"metadata":{"id":"s1irJ0a1J_FY","executionInfo":{"status":"ok","timestamp":1751560757684,"user_tz":-240,"elapsed":502,"user":{"displayName":"Elene Gvalia","userId":"05599052994539163242"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["def load_raw_data():\n","    train = pd.read_csv('data/train.csv')\n","    test = pd.read_csv('data/test.csv')\n","    stores = pd.read_csv('data/stores.csv')\n","    features = pd.read_csv('data/features.csv')\n","    print(f\"Train shape: {train.shape}\")\n","    print(f\"Test shape: {test.shape}\")\n","    print(f\"Features shape: {features.shape}\")\n","    print(f\"Stores shape: {stores.shape}\")\n","    return train, test, stores, features\n","\n","train_raw, test_raw, stores_raw, features_raw = load_raw_data()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fq7pvzafIWWY","executionInfo":{"status":"ok","timestamp":1751564646164,"user_tz":-240,"elapsed":293,"user":{"displayName":"Elene Gvalia","userId":"05599052994539163242"}},"outputId":"ee876662-bec3-4735-9fe1-b2bd910829c3"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Train shape: (421570, 5)\n","Test shape: (115064, 4)\n","Features shape: (8190, 12)\n","Stores shape: (45, 3)\n"]}]},{"cell_type":"markdown","source":["### Data Clearing"],"metadata":{"id":"QyigdISAZfci"}},{"cell_type":"code","source":["class DateParser(BaseEstimator, TransformerMixin):\n","    def fit(self, X, y=None):\n","        return self\n","\n","    def transform(self, X):\n","        X = X.copy()\n","        X['Date'] = pd.to_datetime(X['Date'])\n","        return X"],"metadata":{"id":"5x_UsNi3ZNIT","executionInfo":{"status":"ok","timestamp":1751564752190,"user_tz":-240,"elapsed":6,"user":{"displayName":"Elene Gvalia","userId":"05599052994539163242"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["class DataMerger(BaseEstimator, TransformerMixin):\n","    def __init__(self, stores_df, features_df):\n","        self.stores_df = stores_df.copy()\n","        self.features_df = features_df.copy()\n","        self.features_df['Date'] = pd.to_datetime(self.features_df['Date'])\n","\n","    def fit(self, X, y=None):\n","        return self\n","\n","    def transform(self, X):\n","        X = X.copy()\n","        X = X.merge(self.features_df, on=['Store', 'Date'], how='left')\n","        X = X.merge(self.stores_df, on='Store', how='left')\n","        if 'IsHoliday_x' in X.columns:\n","            X['IsHoliday'] = X['IsHoliday_y'].fillna(X['IsHoliday_x'])\n","            X = X.drop(['IsHoliday_x', 'IsHoliday_y'], axis=1)\n","\n","        return X"],"metadata":{"id":"37mI1A2PZRli","executionInfo":{"status":"ok","timestamp":1751564768078,"user_tz":-240,"elapsed":2,"user":{"displayName":"Elene Gvalia","userId":"05599052994539163242"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["class DataCleaner(BaseEstimator, TransformerMixin):\n","    def __init__(self):\n","        self.cleaning_stats_ = {}\n","\n","    def fit(self, X, y=None):\n","        if 'Weekly_Sales' in X.columns:\n","            self.cleaning_stats_['negative_sales_count'] = (X['Weekly_Sales'] < 0).sum()\n","            self.cleaning_stats_['zero_sales_count'] = (X['Weekly_Sales'] == 0).sum()\n","        return self\n","\n","    def transform(self, X):\n","        X = X.copy()\n","        if 'Weekly_Sales' in X.columns:\n","            X['Weekly_Sales'] = X['Weekly_Sales'].abs()\n","\n","        return X"],"metadata":{"id":"x_uiQ9mNZVcK","executionInfo":{"status":"ok","timestamp":1751564786289,"user_tz":-240,"elapsed":42,"user":{"displayName":"Elene Gvalia","userId":"05599052994539163242"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["### Feature Engineering"],"metadata":{"id":"HR6mMmhYZizZ"}},{"cell_type":"code","source":["class TimeFeatureEngineer(BaseEstimator, TransformerMixin):\n","    def fit(self, X, y=None):\n","        return self\n","\n","    def transform(self, X):\n","        X = X.copy()\n","\n","        X['Year'] = X['Date'].dt.year\n","        X['Month'] = X['Date'].dt.month\n","        X['Week'] = X['Date'].dt.isocalendar().week\n","        X['Day'] = X['Date'].dt.day\n","        X['DayOfWeek'] = X['Date'].dt.dayofweek\n","        X['DayOfYear'] = X['Date'].dt.dayofyear\n","        X['Quarter'] = X['Date'].dt.quarter\n","        X['Month_sin'] = np.sin(2 * np.pi * X['Month'] / 12)\n","        X['Month_cos'] = np.cos(2 * np.pi * X['Month'] / 12)\n","        X['Week_sin'] = np.sin(2 * np.pi * X['Week'] / 52)\n","        X['Week_cos'] = np.cos(2 * np.pi * X['Week'] / 52)\n","        X['WeeksToChristmas'] = (X['DayOfYear'] - 359).abs()\n","        X['WeeksToThanksgiving'] = (X['DayOfYear'] - 327).abs()\n","        X['IsMonthStart'] = (X['Day'] <= 7).astype(int)\n","        X['IsMonthEnd'] = (X['Day'] >= 24).astype(int)\n","\n","        return X"],"metadata":{"id":"l2B15jfJZZzB","executionInfo":{"status":"ok","timestamp":1751564878050,"user_tz":-240,"elapsed":43,"user":{"displayName":"Elene Gvalia","userId":"05599052994539163242"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["class LagFeatureEngineer(BaseEstimator, TransformerMixin):\n","    def __init__(self, lags=[1, 2, 3, 4, 8, 52], windows=[4, 8, 12]):\n","        self.lags = lags\n","        self.windows = windows\n","        self.historical_data_ = None\n","\n","    def fit(self, X, y=None):\n","        if 'Weekly_Sales' in X.columns:\n","            self.historical_data_ = X[['Store', 'Dept', 'Date', 'Weekly_Sales']].copy()\n","            self.historical_data_ = self.historical_data_.sort_values(['Store', 'Dept', 'Date'])\n","        return self\n","\n","    def transform(self, X):\n","        X = X.copy()\n","        X = X.sort_values(['Store', 'Dept', 'Date'])\n","\n","        if 'Weekly_Sales' in X.columns:\n","            for lag in self.lags:\n","                X[f'Sales_Lag_{lag}'] = X.groupby(['Store', 'Dept'])['Weekly_Sales'].shift(lag)\n","\n","            for window in self.windows:\n","                X[f'Sales_MA_{window}'] = X.groupby(['Store', 'Dept'])['Weekly_Sales'].transform(\n","                    lambda x: x.shift(1).rolling(window=window, min_periods=1).mean()\n","                )\n","                X[f'Sales_STD_{window}'] = X.groupby(['Store', 'Dept'])['Weekly_Sales'].transform(\n","                    lambda x: x.shift(1).rolling(window=window, min_periods=1).std()\n","                )\n","        else:\n","            if self.historical_data_ is not None:\n","                # This is where you'd implement logic to create lags from historical data\n","                # For now, initialize with NaN (to be handled by imputation later)\n","                for lag in self.lags:\n","                    X[f'Sales_Lag_{lag}'] = np.nan\n","\n","                for window in self.windows:\n","                    X[f'Sales_MA_{window}'] = np.nan\n","                    X[f'Sales_STD_{window}'] = np.nan\n","\n","                # TODO: Implement proper lag calculation from historical data\n","                # This would involve matching Store/Dept/Date and looking back\n","\n","        return X"],"metadata":{"id":"DzOnshMOZwMa","executionInfo":{"status":"ok","timestamp":1751564936899,"user_tz":-240,"elapsed":42,"user":{"displayName":"Elene Gvalia","userId":"05599052994539163242"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["class StoreDeptFeatureEngineer(BaseEstimator, TransformerMixin):\n","    def __init__(self):\n","        self.store_stats_ = None\n","        self.dept_stats_ = None\n","        self.store_dept_stats_ = None\n","\n","    def fit(self, X, y=None):\n","        if 'Weekly_Sales' in X.columns:\n","            self.store_stats_ = X.groupby('Store').agg({\n","                'Weekly_Sales': ['mean', 'std', 'median'],\n","                'Size': 'first',\n","                'Type': 'first'\n","            })\n","            self.dept_stats_ = X.groupby('Dept').agg({\n","                'Weekly_Sales': ['mean', 'std', 'median']\n","            })\n","            self.store_dept_stats_ = X.groupby(['Store', 'Dept']).agg({\n","                'Weekly_Sales': ['mean', 'std', 'count']\n","            })\n","\n","        return self\n","\n","    def transform(self, X):\n","        X = X.copy()\n","        X['Is_TypeA'] = (X['Type'] == 'A').astype(int)\n","        X['Is_TypeB'] = (X['Type'] == 'B').astype(int)\n","        X['Is_TypeC'] = (X['Type'] == 'C').astype(int)\n","        X['Size_Bin'] = pd.cut(X['Size'],\n","                               bins=[0, 50000, 100000, 150000, 300000],\n","                               labels=['Small', 'Medium', 'Large', 'Extra_Large'])\n","        if self.store_stats_ is not None:\n","            store_means = self.store_stats_['Weekly_Sales']['mean'].to_dict()\n","            store_stds = self.store_stats_['Weekly_Sales']['std'].to_dict()\n","            X['Store_Avg_Sales'] = X['Store'].map(store_means).fillna(0)\n","            X['Store_Std_Sales'] = X['Store'].map(store_stds).fillna(0)\n","            X['Store_CV'] = X['Store_Std_Sales'] / (X['Store_Avg_Sales'] + 1)\n","\n","            dept_means = self.dept_stats_['Weekly_Sales']['mean'].to_dict()\n","            dept_stds = self.dept_stats_['Weekly_Sales']['std'].to_dict()\n","            X['Dept_Avg_Sales'] = X['Dept'].map(dept_means).fillna(0)\n","            X['Dept_Std_Sales'] = X['Dept'].map(dept_stds).fillna(0)\n","\n","        return X"],"metadata":{"id":"lbYb3XqUZ-iw","executionInfo":{"status":"ok","timestamp":1751564974723,"user_tz":-240,"elapsed":48,"user":{"displayName":"Elene Gvalia","userId":"05599052994539163242"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["class MarkdownFeatureEngineer(BaseEstimator, TransformerMixin):\n","    def __init__(self):\n","        self.markdown_cols = ['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']\n","\n","    def fit(self, X, y=None):\n","        return self\n","\n","    def transform(self, X):\n","        X = X.copy()\n","        for col in self.markdown_cols:\n","            if col in X.columns:\n","                X[f'{col}_Present'] = (~X[col].isna()).astype(int)\n","        X['Total_MarkDown'] = X[self.markdown_cols].sum(axis=1, skipna=True)\n","        X['Active_MarkDowns'] = X[[f'{col}_Present' for col in self.markdown_cols\n","                                  if f'{col}_Present' in X.columns]].sum(axis=1)\n","        for col in self.markdown_cols:\n","            if col in X.columns:\n","                X[col] = X[col].fillna(0)\n","\n","        return X"],"metadata":{"id":"XCrlcyYDaHxq","executionInfo":{"status":"ok","timestamp":1751564993773,"user_tz":-240,"elapsed":43,"user":{"displayName":"Elene Gvalia","userId":"05599052994539163242"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["class EconomicFeatureEngineer(BaseEstimator, TransformerMixin):\n","    def __init__(self):\n","        self.impute_values_ = {}\n","\n","    def fit(self, X, y=None):\n","        economic_cols = ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment']\n","        for col in economic_cols:\n","            if col in X.columns:\n","                self.impute_values_[col] = X[col].median()\n","        return self\n","\n","    def transform(self, X):\n","        X = X.copy()\n","        for col, value in self.impute_values_.items():\n","            if col in X.columns:\n","                X[col] = X[col].fillna(value)\n","\n","        if 'Temperature' in X.columns:\n","            X['Temp_Squared'] = X['Temperature'] ** 2\n","            X['Is_Cold'] = (X['Temperature'] < 32).astype(int)\n","            X['Is_Hot'] = (X['Temperature'] > 80).astype(int)\n","\n","        if all(col in X.columns for col in ['Unemployment', 'CPI', 'Fuel_Price']):\n","            X['Economic_Stress'] = (\n","                X['Unemployment'] / X['Unemployment'].mean() +\n","                X['CPI'] / X['CPI'].mean() +\n","                X['Fuel_Price'] / X['Fuel_Price'].mean()\n","            ) / 3\n","\n","        return X"],"metadata":{"id":"LYKYQX78aMef","executionInfo":{"status":"ok","timestamp":1751565012914,"user_tz":-240,"elapsed":3,"user":{"displayName":"Elene Gvalia","userId":"05599052994539163242"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["class CategoricalEncoder(BaseEstimator, TransformerMixin):\n","    def __init__(self):\n","        self.encoders_ = {}\n","        self.categorical_cols_ = ['Size_Bin']\n","\n","    def fit(self, X, y=None):\n","        for col in self.categorical_cols_:\n","            if col in X.columns:\n","                le = LabelEncoder()\n","                # Fit on non-null values\n","                mask = X[col].notna()\n","                if mask.any():\n","                    le.fit(X.loc[mask, col])\n","                    self.encoders_[col] = le\n","        return self\n","\n","    def transform(self, X):\n","        X = X.copy()\n","\n","        for col, encoder in self.encoders_.items():\n","            if col in X.columns:\n","                X[f'{col}_encoded'] = -1  # Default for unseen/missing\n","                mask = X[col].notna()\n","                if mask.any():\n","                    try:\n","                        X.loc[mask, f'{col}_encoded'] = encoder.transform(X.loc[mask, col])\n","                    except ValueError:\n","                        known_values = encoder.classes_\n","                        for idx in X[mask].index:\n","                            if X.loc[idx, col] in known_values:\n","                                X.loc[idx, f'{col}_encoded'] = encoder.transform([X.loc[idx, col]])[0]\n","\n","        return X"],"metadata":{"id":"4a7jVycAaRHy","executionInfo":{"status":"ok","timestamp":1751565029977,"user_tz":-240,"elapsed":5,"user":{"displayName":"Elene Gvalia","userId":"05599052994539163242"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["class FeatureScaler(BaseEstimator, TransformerMixin):\n","    def __init__(self):\n","        self.scaler_ = RobustScaler()\n","        self.numeric_features_ = None\n","\n","    def fit(self, X, y=None):\n","        exclude = ['Store', 'Dept', 'Date', 'Weekly_Sales']\n","        self.numeric_features_ = [col for col in X.select_dtypes(include=[np.number]).columns\n","                                 if col not in exclude]\n","        if self.numeric_features_:\n","            self.scaler_.fit(X[self.numeric_features_])\n","\n","        return self\n","\n","    def transform(self, X):\n","        X = X.copy()\n","        if self.numeric_features_:\n","            X[self.numeric_features_] = self.scaler_.transform(X[self.numeric_features_])\n","\n","        return X"],"metadata":{"id":"65ZCMOpNaVQL","executionInfo":{"status":"ok","timestamp":1751565044679,"user_tz":-240,"elapsed":7,"user":{"displayName":"Elene Gvalia","userId":"05599052994539163242"}}},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":["### Pipeline"],"metadata":{"id":"kRq4jjQJadiZ"}},{"cell_type":"code","source":["class WalmartPreprocessingPipeline(BaseEstimator, TransformerMixin):\n","    def __init__(self, stores_df, features_df):\n","        self.stores_df = stores_df\n","        self.features_df = features_df\n","\n","        self.date_parser = DateParser()\n","        self.data_merger = DataMerger(stores_df, features_df)\n","        self.data_cleaner = DataCleaner()\n","\n","        self.time_feature_engineer = TimeFeatureEngineer()\n","        self.lag_feature_engineer = LagFeatureEngineer()\n","        self.store_dept_engineer = StoreDeptFeatureEngineer()\n","        self.markdown_engineer = MarkdownFeatureEngineer()\n","        self.economic_engineer = EconomicFeatureEngineer()\n","        self.encoder = CategoricalEncoder()\n","        self.scaler = FeatureScaler()\n","        self.is_fitted = False\n","        self.feature_names_ = None\n","        self.numeric_features_ = None\n","        self.categorical_features_ = None\n","\n","    def fit(self, X, y=None):\n","        print(\"Fitting preprocessing pipeline...\")\n","        X_dated = self.date_parser.fit_transform(X)\n","        X_merged = self.data_merger.fit_transform(X_dated)\n","        X_cleaned = self.data_cleaner.fit_transform(X_merged)\n","        X_time = self.time_feature_engineer.fit_transform(X_cleaned)\n","        X_lag = self.lag_feature_engineer.fit_transform(X_time)\n","        X_store = self.store_dept_engineer.fit_transform(X_lag)\n","        X_markdown = self.markdown_engineer.fit_transform(X_store)\n","        X_economic = self.economic_engineer.fit_transform(X_markdown)\n","\n","        X_encoded = self.encoder.fit_transform(X_economic)\n","        X_final = self.scaler.fit_transform(X_encoded)\n","        self.feature_names_ = [col for col in X_final.columns\n","                              if col not in ['Date', 'Weekly_Sales']]\n","        self.numeric_features_ = X_final.select_dtypes(include=[np.number]).columns.tolist()\n","        self.categorical_features_ = X_final.select_dtypes(include=['object']).columns.tolist()\n","\n","        self.is_fitted = True\n","        print(f\"Pipeline fitted. Features created: {len(self.feature_names_)}\")\n","        return self\n","\n","    def transform(self, X):\n","        if not self.is_fitted:\n","            raise ValueError(\"Pipeline must be fitted before transform\")\n","        X_dated = self.date_parser.transform(X)\n","        X_merged = self.data_merger.transform(X_dated)\n","        X_cleaned = self.data_cleaner.transform(X_merged)\n","        X_time = self.time_feature_engineer.transform(X_cleaned)\n","        X_lag = self.lag_feature_engineer.transform(X_time)\n","        X_store = self.store_dept_engineer.transform(X_lag)\n","        X_markdown = self.markdown_engineer.transform(X_store)\n","        X_economic = self.economic_engineer.transform(X_markdown)\n","        X_encoded = self.encoder.transform(X_economic)\n","        X_final = self.scaler.transform(X_encoded)\n","\n","        return X_final\n","\n","    def fit_transform(self, X, y=None):\n","        return self.fit(X, y).transform(X)"],"metadata":{"id":"6cQ8QX5iY7FU","executionInfo":{"status":"ok","timestamp":1751565075448,"user_tz":-240,"elapsed":7,"user":{"displayName":"Elene Gvalia","userId":"05599052994539163242"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["from sklearn.preprocessing import RobustScaler\n","from sklearn.impute import SimpleImputer"],"metadata":{"id":"CIDkkxNYa6nj","executionInfo":{"status":"ok","timestamp":1751565185009,"user_tz":-240,"elapsed":5,"user":{"displayName":"Elene Gvalia","userId":"05599052994539163242"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["pipeline = WalmartPreprocessingPipeline(stores_raw, features_raw)\n","print(\"Fitting pipeline on training data...\")\n","pipeline.fit(train_raw)\n","print(\"\\nTransforming training data...\")\n","train_processed = pipeline.transform(train_raw)\n","print(f\"Train processed shape: {train_processed.shape}\")\n","\n","print(\"\\nTransforming test data...\")\n","test_processed = pipeline.transform(test_raw)\n","print(f\"Test processed shape: {test_processed.shape}\")\n","\n","print(f\"\\nFeatures in train: {train_processed.shape[1]}\")\n","print(f\"Features in test: {test_processed.shape[1]}\")\n","print(f\"Weekly_Sales in test: {'Weekly_Sales' in test_processed.columns}\")  # Should be False"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n9k6HU8-agXo","executionInfo":{"status":"ok","timestamp":1751565219154,"user_tz":-240,"elapsed":33478,"user":{"displayName":"Elene Gvalia","userId":"05599052994539163242"}},"outputId":"4baed37b-1b9c-4c88-a5c9-e99b3def75e5"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Fitting pipeline on training data...\n","Fitting preprocessing pipeline...\n","Pipeline fitted. Features created: 62\n","\n","Transforming training data...\n","Train processed shape: (421570, 64)\n","\n","Transforming test data...\n","Test processed shape: (115064, 63)\n","\n","Features in train: 64\n","Features in test: 63\n","Weekly_Sales in test: False\n"]}]}]}